{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 17777,
     "databundleVersionId": 869809,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30762,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Natural Language Processing with Disaster Tweets\n\nIn this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.",
   "metadata": {},
   "id": "898ea7d6f8c07fef"
  },
  {
   "cell_type": "markdown",
   "source": "Different things to keep in mind compare to main.ipynb:\n- Use all the columns\n- Processing pipeline (lowercasing, stopword removal, punctuation removal, lemmatization, tokenization, and padding)\n- Use ML classification algorithms",
   "metadata": {},
   "id": "6a1f27be1efcca9"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import contractions\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T17:03:58.514721Z",
     "iopub.execute_input": "2024-09-02T17:03:58.515086Z",
     "iopub.status.idle": "2024-09-02T17:04:20.223362Z",
     "shell.execute_reply.started": "2024-09-02T17:03:58.515038Z",
     "shell.execute_reply": "2024-09-02T17:04:20.222293Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-09-03T11:53:24.717570Z",
     "start_time": "2024-09-03T11:53:24.711950Z"
    }
   },
   "id": "5d001f7a5ff9090a",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "kaggle_run = False\n",
    "if kaggle_run:\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "    train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "    test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "else:\n",
    "    train = pd.read_csv('data/train.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    submission = pd.read_csv('data/sample_submission.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T17:04:29.364904Z",
     "iopub.execute_input": "2024-09-02T17:04:29.366133Z",
     "iopub.status.idle": "2024-09-02T17:04:29.470082Z",
     "shell.execute_reply.started": "2024-09-02T17:04:29.366071Z",
     "shell.execute_reply": "2024-09-02T17:04:29.468774Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-09-03T11:53:25.035762Z",
     "start_time": "2024-09-03T11:53:25.005296Z"
    }
   },
   "id": "3e74e34b7e653449",
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "metadata": {},
   "id": "ff7b22bbff1a5ef1"
  },
  {
   "cell_type": "code",
   "source": [
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def preprocessing(df):\n",
    "    df.fillna('', inplace=True)\n",
    "    \n",
    "    # Maybe apply Textblob machine learning algorithm for correction of misspelled words.\n",
    "        \n",
    "    df['combined_text'] = df['keyword'] + ' ' + df['text']\n",
    "    df = df.drop(['id','keyword','location','text'], axis=1)\n",
    "        \n",
    "    df['combined_text'] = df['combined_text'].str.lower()\n",
    "    df['combined_text'] = df['combined_text'].apply(lambda x: re.sub(r'http[s]?://\\S+|www\\.\\S+','', x))\n",
    "\n",
    "    #df['combined_text'] = df['combined_text'].apply(lambda x: re.sub(r'@\\w+', '', x))\n",
    "    df['combined_text'] = df['combined_text'].apply(lambda x: re.sub(r'@[^ \\t\\n\\r\\f\\v]+', '', x))\n",
    "    df['combined_text'] = df['combined_text'].str.replace(' @ ', '', regex=False)\n",
    "\n",
    "    df['combined_text'] = df['combined_text'].apply(expand_contractions)\n",
    "    \n",
    "    df['combined_text'] = df['combined_text'].apply(remove_numbers)\n",
    "    \n",
    "    df['combined_text'] = df['combined_text'].str.replace('#', '', regex=False)\n",
    "    df['combined_text'] = df['combined_text'].str.replace('\"', '', regex=False)\n",
    "    df['combined_text'] = df['combined_text'].str.replace(' \"', '', regex=False)\n",
    "    df['combined_text'] = df['combined_text'].str.replace(' \" ', '', regex=False)\n",
    "    df['combined_text'] = df['combined_text'].str.replace('\" ', '', regex=False)\n",
    "    df['combined_text'] = df['combined_text'].str.replace(' | ', '', regex=False)\n",
    "    df['combined_text'] = df['combined_text'].str.replace('+', '', regex=False)\n",
    "    df['combined_text'] = df['combined_text'].str.replace('*', '', regex=False)\n",
    "    df['combined_text'] = df['combined_text'].str.replace(' via ', '', regex=False)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T17:04:35.306120Z",
     "iopub.execute_input": "2024-09-02T17:04:35.306912Z",
     "iopub.status.idle": "2024-09-02T17:04:35.312717Z",
     "shell.execute_reply.started": "2024-09-02T17:04:35.306865Z",
     "shell.execute_reply": "2024-09-02T17:04:35.311340Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-09-03T11:53:26.864111Z",
     "start_time": "2024-09-03T11:53:26.855172Z"
    }
   },
   "id": "af17177d0f71ea9f",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "train = preprocessing(train)\n",
    "test = preprocessing(test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T17:04:36.067921Z",
     "iopub.execute_input": "2024-09-02T17:04:36.068798Z",
     "iopub.status.idle": "2024-09-02T17:04:36.092760Z",
     "shell.execute_reply.started": "2024-09-02T17:04:36.068759Z",
     "shell.execute_reply": "2024-09-02T17:04:36.092048Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-09-03T11:53:27.998790Z",
     "start_time": "2024-09-03T11:53:27.887187Z"
    }
   },
   "id": "6e066faa754df26c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T12:08:33.096833Z",
     "start_time": "2024-09-03T11:53:31.586093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train['combined_text'].tolist(), \n",
    "    train['target'].tolist(), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(len(train_texts)).batch(16)\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    val_texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "\n",
    "val_dataset = val_dataset.batch(16)\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=3e-5,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test['combined_text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings)))\n",
    "test_dataset = test_dataset.batch(16)\n",
    "\n",
    "predictions = model.predict(test_dataset)"
   ],
   "id": "c7f1d665d9acd032",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381/381 [==============================] - 788s 2s/step - loss: 0.4427 - accuracy: 0.8097 - val_loss: 0.3997 - val_accuracy: 0.8352\n",
      "204/204 [==============================] - 109s 531ms/step\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T12:09:04.979497Z",
     "start_time": "2024-09-03T12:09:04.976630Z"
    }
   },
   "cell_type": "code",
   "source": "print(predictions)",
   "id": "4bb4bf4958bfb0ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFSequenceClassifierOutput(loss=None, logits=array([[-0.57171196,  0.9919148 ],\n",
      "       [-0.52915317,  0.6366399 ],\n",
      "       [-0.85720074,  1.1818699 ],\n",
      "       ...,\n",
      "       [-1.295325  ,  1.4281175 ],\n",
      "       [-0.7490389 ,  1.11756   ],\n",
      "       [-0.0909982 ,  0.02976773]], dtype=float32), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare data",
   "id": "efa84befd4f59908"
  },
  {
   "cell_type": "code",
   "source": [
    "# ----- Train preprocess ------\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenized_data = tokenizer(\n",
    "    train['combined_text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "labels = train['target'].tolist()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(tokenized_data), \n",
    "    labels \n",
    "))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(len(train)).batch(16)\n",
    "\n",
    "# ----- Test preprocess ------\n",
    "\n",
    "test = preprocessing(test)\n",
    "\n",
    "test = tokenizer(\n",
    "    test['combined_text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test = tf.data.Dataset.from_tensor_slices((dict(test)))\n",
    "test = test.batch(16)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T17:04:36.919501Z",
     "iopub.execute_input": "2024-09-02T17:04:36.919872Z",
     "iopub.status.idle": "2024-09-02T17:04:45.565023Z",
     "shell.execute_reply.started": "2024-09-02T17:04:36.919835Z",
     "shell.execute_reply": "2024-09-02T17:04:45.563970Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-09-02T22:37:59.197309Z",
     "start_time": "2024-09-02T22:37:57.098100Z"
    }
   },
   "id": "6454922652026a72",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": "## Build model",
   "metadata": {},
   "id": "c790e073500224da"
  },
  {
   "cell_type": "code",
   "source": "# Load BERT model for sequence classification\nmodel = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Compile the model\nmodel.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\n# Train the model\nmodel.fit(train_dataset, epochs=10)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T15:44:34.241587Z",
     "start_time": "2024-09-02T15:44:01.154884Z"
    },
    "execution": {
     "iopub.status.busy": "2024-09-02T17:04:59.369907Z",
     "iopub.execute_input": "2024-09-02T17:04:59.370757Z",
     "iopub.status.idle": "2024-09-02T17:30:58.660839Z",
     "shell.execute_reply.started": "2024-09-02T17:04:59.370717Z",
     "shell.execute_reply": "2024-09-02T17:30:58.659820Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/10\nWARNING: AutoGraph could not transform <function infer_framework at 0x7e374c259ea0> and will run it as-is.\nCause: for/else statement not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1725296783.571265     106 service.cc:145] XLA service 0x7e36f07cd710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1725296783.571325     106 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1725296783.571331     106 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1725296783.748529     106 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "476/476 [==============================] - 238s 304ms/step - loss: 0.7155 - accuracy: 0.5347\nEpoch 2/10\n476/476 [==============================] - 147s 308ms/step - loss: 0.6940 - accuracy: 0.5491\nEpoch 3/10\n476/476 [==============================] - 147s 308ms/step - loss: 0.6902 - accuracy: 0.5576\nEpoch 4/10\n476/476 [==============================] - 147s 308ms/step - loss: 0.6890 - accuracy: 0.5611\nEpoch 5/10\n476/476 [==============================] - 147s 308ms/step - loss: 0.6863 - accuracy: 0.5684\nEpoch 6/10\n476/476 [==============================] - 147s 308ms/step - loss: 0.6854 - accuracy: 0.5677\nEpoch 7/10\n476/476 [==============================] - 147s 308ms/step - loss: 0.6850 - accuracy: 0.5688\nEpoch 8/10\n476/476 [==============================] - 147s 308ms/step - loss: 0.6857 - accuracy: 0.5694\nEpoch 9/10\n476/476 [==============================] - 147s 308ms/step - loss: 0.6902 - accuracy: 0.5560\nEpoch 10/10\n476/476 [==============================] - 147s 308ms/step - loss: 0.6847 - accuracy: 0.5702\n",
     "output_type": "stream"
    },
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf_keras.src.callbacks.History at 0x7e37477ae110>"
     },
     "metadata": {}
    }
   ],
   "id": "f8f028843405012a"
  },
  {
   "cell_type": "markdown",
   "source": "## Prediction on new data ",
   "metadata": {},
   "id": "fd2f41f703856ed"
  },
  {
   "cell_type": "code",
   "source": [
    "#predictions = model.predict(test)\n",
    "#print(f\"Direct model predictions: \\n {predictions}\")\n",
    "\n",
    "probabilities = tf.nn.softmax(predictions.logits, axis=-1)\n",
    "print(f\"Probabilities \\n {probabilities}\")\n",
    "\n",
    "predicted_classes = tf.argmax(probabilities, axis=1).numpy()\n",
    "#final_predictions = tf.where(probabilities >= 0.5, 1.0, 0.0)\n",
    "\n",
    "print(f\"Predicted classes \\n {predicted_classes}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T17:38:42.768817Z",
     "iopub.execute_input": "2024-09-02T17:38:42.769218Z",
     "iopub.status.idle": "2024-09-02T17:39:02.351322Z",
     "shell.execute_reply.started": "2024-09-02T17:38:42.769178Z",
     "shell.execute_reply": "2024-09-02T17:39:02.350236Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-09-03T12:09:21.761026Z",
     "start_time": "2024-09-03T12:09:21.755235Z"
    }
   },
   "id": "74cfc2f5f1a22a61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities \n",
      " [[0.17312683 0.8268731 ]\n",
      " [0.23761626 0.76238376]\n",
      " [0.1151614  0.8848386 ]\n",
      " ...\n",
      " [0.06160416 0.9383959 ]\n",
      " [0.13393575 0.86606425]\n",
      " [0.46984515 0.5301548 ]]\n",
      "Predicted classes \n",
      " [1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": "## Prepare upload",
   "metadata": {},
   "id": "e347b9935e06e8ed"
  },
  {
   "cell_type": "code",
   "source": [
    "choosen_model_name = 'bert_e1_new_attempt'\n",
    "choosen_model_predictions = predicted_classes\n",
    "\n",
    "now = datetime.now()\n",
    "date_time_str = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "if kaggle_run:\n",
    "    submission = pd.DataFrame({\n",
    "        'id': pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')['id'],\n",
    "        'target': choosen_model_predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv(f'/kaggle/working/submission_{choosen_model_name}_{date_time_str}.csv', index=False)\n",
    "else:\n",
    "    submission = pd.DataFrame({\n",
    "        'id': pd.read_csv('data/test.csv')['id'],\n",
    "        'target': choosen_model_predictions\n",
    "    })\n",
    "    submission.to_csv(f'output/submission_{choosen_model_name}_{date_time_str}.csv', index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T17:41:47.246091Z",
     "iopub.execute_input": "2024-09-02T17:41:47.247174Z",
     "iopub.status.idle": "2024-09-02T17:41:47.277320Z",
     "shell.execute_reply.started": "2024-09-02T17:41:47.247127Z",
     "shell.execute_reply": "2024-09-02T17:41:47.276203Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-09-03T12:09:33.626911Z",
     "start_time": "2024-09-03T12:09:33.612887Z"
    }
   },
   "id": "927656f6262f0905",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "3af6f532d0c14539"
  }
 ]
}
