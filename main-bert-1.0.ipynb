{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing with Disaster Tweets\n\nIn this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.","metadata":{}},{"cell_type":"markdown","source":"Different things to keep in mind compare to main.ipynb:\n- Use all the columns\n- Processing pipeline (lowercasing, stopword removal, punctuation removal, lemmatization, tokenization, and padding)\n- Use ML classification algorithms","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nimport numpy as np\n\nimport os\n\nimport re\n\n#from .autonotebook import tqdm as notebook_tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\n\nfrom transformers import BertTokenizer\nfrom transformers import TFBertForSequenceClassification\n\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score","metadata":{"ExecuteTime":{"end_time":"2024-09-02T15:11:33.351644Z","start_time":"2024-09-02T15:11:27.279213Z"},"execution":{"iopub.status.busy":"2024-09-02T20:06:41.921529Z","iopub.execute_input":"2024-09-02T20:06:41.922129Z","iopub.status.idle":"2024-09-02T20:06:41.928176Z","shell.execute_reply.started":"2024-09-02T20:06:41.922092Z","shell.execute_reply":"2024-09-02T20:06:41.927289Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"kaggle_run = True\nif kaggle_run:\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n    test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nelse:\n    train = pd.read_csv('data/train.csv')\n    test = pd.read_csv('data/test.csv')\n    submission = pd.read_csv('data/sample_submission.csv')","metadata":{"ExecuteTime":{"end_time":"2024-09-02T15:11:33.375437Z","start_time":"2024-09-02T15:11:33.352512Z"},"execution":{"iopub.status.busy":"2024-09-02T20:06:44.384122Z","iopub.execute_input":"2024-09-02T20:06:44.384995Z","iopub.status.idle":"2024-09-02T20:06:44.453931Z","shell.execute_reply.started":"2024-09-02T20:06:44.384950Z","shell.execute_reply":"2024-09-02T20:06:44.452955Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocessing(df):\n    df.fillna('', inplace=True)\n    \n    df['combined_text'] = df['keyword'] + ' ' + df['location'] + ' ' + df['text']\n    df = df.drop(['id','keyword','location','text'], axis=1)\n    return df","metadata":{"ExecuteTime":{"end_time":"2024-09-02T15:11:33.378092Z","start_time":"2024-09-02T15:11:33.376070Z"},"execution":{"iopub.status.busy":"2024-09-02T20:06:46.024957Z","iopub.execute_input":"2024-09-02T20:06:46.025561Z","iopub.status.idle":"2024-09-02T20:06:46.030916Z","shell.execute_reply.started":"2024-09-02T20:06:46.025522Z","shell.execute_reply":"2024-09-02T20:06:46.029842Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train = preprocessing(train)","metadata":{"ExecuteTime":{"end_time":"2024-09-02T15:11:33.385257Z","start_time":"2024-09-02T15:11:33.379334Z"},"execution":{"iopub.status.busy":"2024-09-02T20:06:46.540945Z","iopub.execute_input":"2024-09-02T20:06:46.541351Z","iopub.status.idle":"2024-09-02T20:06:46.566756Z","shell.execute_reply.started":"2024-09-02T20:06:46.541311Z","shell.execute_reply":"2024-09-02T20:06:46.565735Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# ----- Train preprocess ------\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntokenized_data = tokenizer(\n    train['combined_text'].tolist(),\n    padding=True,\n    truncation=True,\n    return_tensors='tf'\n)\n\nlabels = train['target'].tolist()\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(tokenized_data), \n    labels\n))\n\ntrain_dataset = train_dataset.shuffle(len(train)).batch(16)\n\n# ----- Test preprocess ------\n\ntest = preprocessing(test)\n\ntest = tokenizer(\n    test['combined_text'].tolist(),\n    padding=True,\n    truncation=True,\n    return_tensors='tf'\n)\n\ntest = tf.data.Dataset.from_tensor_slices((dict(test)))\ntest = test.batch(16)","metadata":{"ExecuteTime":{"end_time":"2024-09-02T15:11:35.210909Z","start_time":"2024-09-02T15:11:33.386407Z"},"execution":{"iopub.status.busy":"2024-09-02T20:06:47.125239Z","iopub.execute_input":"2024-09-02T20:06:47.125624Z","iopub.status.idle":"2024-09-02T20:06:58.752794Z","shell.execute_reply.started":"2024-09-02T20:06:47.125585Z","shell.execute_reply":"2024-09-02T20:06:58.751910Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a8281406b9b435e98066f5776fa43de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a25d217d4f471daf1ae712fed7062e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"423705762fff41f9948134ce8a7c438f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef90647ce48f41d582076419154427cb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Build model","metadata":{}},{"cell_type":"code","source":"# Load BERT model for sequence classification\nmodel = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Compile the model\nmodel.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\n# Train the model\nmodel.fit(train_dataset, epochs=20)","metadata":{"ExecuteTime":{"end_time":"2024-09-02T15:44:34.241587Z","start_time":"2024-09-02T15:44:01.154884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction on new data ","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(test)\nprint(f\"Direct model predictions: \\n {predictions}\")\n\nprobabilities = tf.nn.softmax(predictions.logits, axis=-1)\nprint(f\"Probabilities \\n {probabilities}\")\n\npredicted_classes = tf.argmax(probabilities, axis=1).numpy()\n#final_predictions = tf.where(probabilities >= 0.5, 1.0, 0.0)\n\nprint(f\"Predicted classes \\n {predicted_classes}\")","metadata":{"ExecuteTime":{"end_time":"2024-09-02T15:40:53.342816Z","start_time":"2024-09-02T15:38:35.321415Z"},"execution":{"iopub.status.busy":"2024-09-02T17:38:42.768817Z","iopub.execute_input":"2024-09-02T17:38:42.769218Z","iopub.status.idle":"2024-09-02T17:39:02.351322Z","shell.execute_reply.started":"2024-09-02T17:38:42.769178Z","shell.execute_reply":"2024-09-02T17:39:02.350236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare upload","metadata":{}},{"cell_type":"code","source":"choosen_model_name = 'bert_e10'\nchoosen_model_predictions = predicted_classes\n\nnow = datetime.now()\ndate_time_str = now.strftime(\"%Y%m%d_%H%M%S\")\n\nif kaggle_run:\n    submission = pd.DataFrame({\n        'id': pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')['id'],\n        'target': choosen_model_predictions\n    })\n    print(submission)\n    print(submission['target'])\n\n    submission.to_csv(f'/kaggle/working/submission_{choosen_model_name}_{date_time_str}.csv', index=False)\nelse:\n    submission = pd.DataFrame({\n        'id': pd.read_csv('data/test.csv')['id'],\n        'target': choosen_model_predictions\n    })\n    submission.to_csv(f'output/submission_{choosen_model_name}_{date_time_str}.csv', index=False)","metadata":{"ExecuteTime":{"end_time":"2024-09-02T15:42:56.526571Z","start_time":"2024-09-02T15:42:56.507084Z"},"execution":{"iopub.status.busy":"2024-09-02T17:41:47.246091Z","iopub.execute_input":"2024-09-02T17:41:47.247174Z","iopub.status.idle":"2024-09-02T17:41:47.277320Z","shell.execute_reply.started":"2024-09-02T17:41:47.247127Z","shell.execute_reply":"2024-09-02T17:41:47.276203Z"},"trusted":true},"execution_count":null,"outputs":[]}]}